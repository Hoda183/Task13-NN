import numpy as np

class NeuralNetwork:
    def __init__(self, input_size, hidden_size, output_size):
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # Initialize weights and biases
        self.W1 = np.random.randn(self.input_size, self.hidden_size) / np.sqrt(self.input_size)
        self.b1 = np.zeros((1, self.hidden_size))
        self.W2 = np.random.randn(self.hidden_size, self.output_size) / np.sqrt(self.hidden_size)
        self.b2 = np.zeros((1, self.output_size))
    
    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))
    
    def sigmoid_derivative(self, x):
        return x * (1 - x)
    
    def softmax(self, x):
        exp_scores = np.exp(x)
        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
    
    def forward(self, X):
        self.z1 = np.dot(X, self.W1) + self.b1
        self.a1 = self.sigmoid(self.z1)
        self.z2 = np.dot(self.a1, self.W2) + self.b2
        self.a2 = self.softmax(self.z2)
        return self.a2
    
    def backward(self, X, y, output):
        self.output_error = output - y
        self.output_delta = self.output_error
        
        self.z2_error = np.dot(self.output_delta, self.W2.T)
        self.z2_delta = self.z2_error * self.sigmoid_derivative(self.a1)
        
        self.W2 -= self.learning_rate * np.dot(self.a1.T, self.output_delta)
        self.b2 -= self.learning_rate * np.sum(self.output_delta, axis=0, keepdims=True)
        self.W1 -= self.learning_rate * np.dot(X.T, self.z2_delta)
        self.b1 -= self.learning_rate * np.sum(self.z2_delta, axis=0, keepdims=True)
    
    def train(self, X, y, epochs, learning_rate):
        self.learning_rate = learning_rate
        for _ in range(epochs):
            output = self.forward(X)
            self.backward(X, y, output)
    
    def predict(self, X):
        return np.argmax(self.forward(X), axis=1)

# Load and preprocess MNIST data
def load_mnist():
    # Placeholder for loading MNIST data
    # In practice, you would use a library like mnist to load the actual data
    X_train = np.random.rand(60000, 784)
    y_train = np.random.randint(0, 10, 60000)
    X_test = np.random.rand(10000, 784)
    y_test = np.random.randint(0, 10, 10000)
    return (X_train, y_train), (X_test, y_test)

# Train and evaluate the model
(X_train, y_train), (X_test, y_test) = load_mnist()

# Normalize the data
X_train = X_train / 255.0
X_test = X_test / 255.0

# Convert labels to one-hot encoded format
y_train_onehot = np.eye(10)[y_train]

# Initialize and train the neural network
nn = NeuralNetwork(input_size=784, hidden_size=128, output_size=10)
nn.train(X_train, y_train_onehot, epochs=10, learning_rate=0.1)

# Evaluate the model
predictions = nn.predict(X_test)
accuracy = np.mean(predictions == y_test)
print(f"Accuracy: {accuracy * 100:.2f}%")
